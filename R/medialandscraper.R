#' Title
#'
#' @param outlets Vector of media-outlets that should be scraped. Currently supported: Watson.ch, 20 Minuten.ch
#' @param browser Browser that should be used for scraping. Default: Firefox
#' @param port Port that should be used for scraping. Default: 4491L
#' @param sqldb Whether scraping-results should to be stored in an sql-database or not. Default: FALSE
#' @param dbname Name for the sql-database if sqldb = TRUE
#' @param plots Provides a summary of the scraped data in the form of basic plots if TRUE. Plots show: Number of articles per outlet scraped and mean-length of title/lead/body of the articles that where scraped. Default: FALSE
#' @param searchterm Regular expression for which you want to search in the titles of the scraped articles. Result is a plot
#'
#' @return R-dataframe or sql-database
#' @export
#'
#' @examples
#' mediascraper(outlets = c("Watson", "20 Minuten"), plots = TRUE)
mediascraper = function(outlets, browser = "firefox", port = 4491L, sqldb = FALSE,
                        dbname, plots = FALSE, searchterm = NULL){

  suppressMessages(require(RSelenium))
  suppressMessages(require(stringr))
  suppressMessages(require(dplyr))
  suppressMessages(require(ggplot2))


  # Checking function-inputs
  if (any(outlets %in% c("Watson", "SRF", "20 Minuten") == FALSE)){
    stop("Error: One of the newsoutlets you specified is not (yet) supported")
  }




  # Initiating sql-database (if requested by the user)
  if (sqldb == TRUE){require(DBI)
    con = DBI::dbConnect(RSQLite::SQLite(), dbname)
    dbSendQuery(con, "CREATE TABLE scrapingresults(
              title TEXT,
              lead TEXT,
              body TEXT,
              url VARCHAR(128),
              outlet VARCHAR(32) NOT NULL,
              time DATETIME
            )")
    DBI::dbListTables(con)
  }



  # List to store results
  df_list <- list()




  # Initiate scraper
  cat("Initiating Webscraper\n")
  driver = suppressMessages(RSelenium::rsDriver(browser=browser, port = port, chromever = NULL))
  rs = suppressMessages(driver$client)






  # Watson
  if ("Watson" %in% outlets){
    cat("Started scraping Watson\n")

    suppressMessages(rs$navigate("https://watson.ch"))
    Sys.sleep(10)

    html = rs$getPageSource() # get source of the page
    links = stringr::str_match_all(html, '<a class="watson-teaser__link[^"]*" href=\"(.*?)\"')[[1]]
    links = as.data.frame(links)[,2]

    # Delete links that do not begin with "https://www.watson.ch"
    links <- links[grep("^https://www.watson.ch", links)]


    df_watson = data.frame(title = rep(NA_character_, length(links)),
                           lead = rep(NA_character_, length(links)),
                           body = rep(NA_character_, length(links)),
                           url = rep(NA_character_, length(links)),
                           outlet = rep("Watson", length(links)),
                           time = rep(Sys.time(), length(links)))


    for(i in seq_along(links)){
      suppressMessages(rs$navigate(links[i]))
      # css: id: #id, class: .class, tag: tag
      title_e = rs$findElements(using = 'css selector', 'h2')
      title = title_e[[1]]$getElementText()[[1]]
      title = gsub("'", "", title) # escaping single quotations
      df_watson[i, "title"] = title
      lead_e = rs$findElements(using = 'css selector', '.watson-snippet__lead')
      if(length(lead_e)>0){
        lead = lead_e[[1]]$getElementText()[[1]]
        lead = gsub("'", "", lead)
        df_watson[i, "lead"] = lead
      }


      body_e = rs$findElements(using = 'css selector', 'p')
      if (length(body_e) > 0) {
        body_text = sapply(body_e, function(elem) elem$getElementText()[[1]])
        body = paste(body_text, collapse = " ")
        body = gsub("'", "", body)
        df_watson[i, "body"] = body
      }

      time = Sys.time()
      link = links[i]
      df_watson[i, "time"] = time
      df_watson[i, "url"] = link



      if (sqldb == TRUE){sql = paste0("INSERT INTO scrapingresults VALUES ('",title,"','",lead,"','",body,"','",link,"','Watson','",time,"')")
      suppressMessages(DBI::dbSendStatement(con, sql))}


    }

    df_list[["Watson"]] <- df_watson

  }



  # 20 Minuten
  if ("20 Minuten" %in% outlets){
    cat("Started scraping 20 Minuten\n")

    rs$navigate("https://20min.ch/")
    Sys.sleep(10)

    html = rs$getPageSource() # get source of the page
    links = stringr::str_match_all(html, '<a class="sc-bb81291f-1[^"]*" href=\"(.*?)\"')[[1]]
    links = as.data.frame(links)[,2]


    # Links have to be corrected because some of them do not begin with https://www.20min.ch
    indices <- !grepl("^https://", links)
    links[indices] <- paste0("https://www.20min.ch", links[indices])


    df_20minuten = data.frame(title = rep(NA_character_, length(links)),
                              lead = rep(NA_character_, length(links)),
                              body = rep(NA_character_, length(links)),
                              url = rep(NA_character_, length(links)),
                              outlet = rep("20 Minuten", length(links)),
                              time = rep(Sys.time(), length(links)))


    for(i in seq_along(links)){
      suppressMessages(rs$navigate(links[i]))
      # css: id: #id, class: .class, tag: tag
      title_e = rs$findElements(using = 'css selector', 'h2')
      title = title_e[[1]]$getElementText()[[1]]
      title = gsub("'", "", title) # escaping single quotations
      df_20minuten[i, "title"] = title

      lead_e = rs$findElements(using = 'css selector', '.Article_elementLead__a52sm')
      if(length(lead_e)>0){
        lead = lead_e[[1]]$getElementText()[[1]]
        lead = gsub("'", "", lead)
        df_20minuten[i, "lead"] = lead
      }


      body_e = rs$findElements(using = 'css selector', 'p')
      if (length(body_e) > 0) {
        body_text = sapply(body_e, function(elem) elem$getElementText()[[1]])
        body = paste(body_text, collapse = " ")
        body = gsub("'", "", body)
        df_20minuten[i, "body"] = body
      }

      time = Sys.time()
      link = links[i]
      df_20minuten[i, "time"] = time
      df_20minuten[i, "url"] = link




      if (sqldb == TRUE){sql = paste0("INSERT INTO scrapingresults VALUES ('",title,"','",lead,"','",body,"','",link,"','20 Minuten','",time,"')")
      suppressMessages(DBI::dbSendStatement(con, sql))}


    }

    df_list[["20 Minuten"]] <- df_20minuten

  }





  # Create results-dataframe
  results_df <- do.call(rbind, df_list)
  rownames(results_df) <- NULL





  # Writing code to create analysis plots:
  # Number of articles
  if (plots == TRUE){

    # Setting colors for the different outlets
    colors <- c("Watson" = "#F40F96", "20 Minuten" = "#0D2880", "SRF" = "#AF001D")

    nrart <- results_df |> dplyr::group_by(outlet) |>
      dplyr::summarize(n = n()) |> ggplot(aes(x = outlet, y = n, color = outlet, fill =
                                                outlet)) +
      geom_col(width = 0.5, show.legend = F) +
      xlab("Outlet") + ylab("Number of Articles")  +
      ggtitle("Number of Scrapped Articles per Outlet") + theme_bw() +
      scale_fill_manual(values = colors) + scale_color_manual(values = colors)
    print(nrart)

    # Length of title
    lete <- results_df |> dplyr::group_by(outlet) |>
      dplyr::summarize(titlelength = mean(nchar(title), na.rm = T)) |> ggplot(aes(x = outlet,
                                                                                  y = titlelength,
                                                                                  color = outlet,
                                                                                  fill = outlet)) +
      geom_col(width = 0.5, show.legend = F) + xlab("Outlet") +
      ylab("Number of Characters")  + ggtitle("Mean Length of Article-Title") +
      theme_bw() + scale_fill_manual(values = colors) + scale_color_manual(values = colors)
    print(lete)

    # Length of lead
    lele <- results_df |> dplyr::group_by(outlet) |>
      dplyr::summarize(leadlength = mean(nchar(lead), na.rm = T)) |> ggplot(aes(x = outlet,
                                                                                y = leadlength,
                                                                                color = outlet,
                                                                                fill = outlet)) +
      geom_col(width = 0.5, show.legend = F) + xlab("Outlet") +
      ylab("Number of Characters")  +
      ggtitle("Mean Length of Article-Lead") + theme_bw() + scale_fill_manual(values = colors)  + scale_color_manual(values = colors)
    print(lele)

    # Length of body
    lebo <- results_df |> dplyr::group_by(outlet) |>
      dplyr::summarize(bodylength = mean(nchar(body), na.rm = T)) |> ggplot(aes(x = outlet,
                                                                                y = bodylength,
                                                                                color = outlet,
                                                                                fill = outlet)) +
      geom_col(width = 0.5, show.legend = F) + xlab("Outlet") +
      ylab("Number of Characters")  + ggtitle("Mean Length of Article-Body") +
      theme_bw() + scale_fill_manual(values = colors)  + scale_color_manual(values = colors)
    print(lebo)
  }


  # Search for appearances of a specific word in the title of the articles
  if (!is.null(searchterm)){
    wordo <- results_df  |>
      dplyr::mutate(word_count = str_count(title, searchterm)) |> dplyr::group_by(outlet) |>
      dplyr::summarise(
        word_count = sum(word_count)
      ) |> ggplot(aes(x = outlet, y = word_count, color = outlet, fill = outlet)) +
      geom_col(width = 0.5, show.legend = F) + xlab("Outlet") +
      ylab("Number of Appearances")  +
      ggtitle("Number of Appearances of Word Specified") + theme_bw()
    print(wordo)}



  # Saving results into an sql-database or returning r-dataframe
  rs$close()
  if (sqldb == TRUE){
    cat("Done. Saved results in sql-database 'scrapingresults'. Use object 'con' in environment to connect to database\n")
    assign("con", con, envir = .GlobalEnv)

  } else{
    cat("Done. Returned r-dataframe\n")
    assign("scrapingresults", results_df, envir = .GlobalEnv)
    return(results_df)}

}
